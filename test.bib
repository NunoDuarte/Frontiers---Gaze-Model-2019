@Article{gallotti2017alignment,
  title={Alignment in social interactions},
  author={Gallotti, Mattia and Fairhurst, MT and Frith, CD},
  journal={Consciousness and cognition},
  volume={48},
  pages={253--261},
  year={2017},
  publisher={Elsevier}
}

@ARTICLE{duarte2018action,
author={N. F. Duarte and M. Rakovi\'{c} and J. Tasevski and M. I. Coco and A. Billard and J. Santos-Victor},
journal={IEEE Robotics and Automation Letters},
title={Action Anticipation: Reading the Intentions of Humans and Robots},
year={2018},
volume={3},
number={4},
pages={4132-4139},
keywords={control engineering;gaze tracking;human computer interaction;humanoid robots;robot vision;sensor fusion;visual perception;action anticipation;nonverbal visual cues;intention reading ability;shared motor repertoires;action models;human body motion;eye gaze;action perception;robot intentions;human intentions;nonverbal cues;motion recordings;gaze recordings;iCub humanoid robot controller;sensor fusion;Robot kinematics;Computational modeling;Robot sensing systems;Solid modeling;Tracking;Magnetic heads;Social human-robot interaction;humanoid robots;sensor fusion},
doi={10.1109/LRA.2018.2861569},
ISSN={2377-3766},
month={Oct},}

@INPROCEEDINGS{dragan2013legibility, 
author={A. D. Dragan and K. C. T. Lee and S. S. Srinivasa}, 
booktitle={2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
title={Legibility and predictability of robot motion}, 
year={2013}, 
volume={}, 
number={}, 
pages={301-308}, 
keywords={human-robot interaction;motion control;optimisation;path planning;trajectory control;action interpretation;collaborative robot motion;cost optimization;expected motion;goal inference;human collaborator;intent-expressive motion;mathematical model;motion planning;predictable motion;psychology;rational action;robot intention;robot motion legibility;robot motion predictability;seamless human-robot collaboration;trajectory inference;unsurprising motion;Collaboration;IP networks;Observers;Robot motion;Trajectory;Videos;action interpretation;formalism;human-robot collaboration;manipulation;motion planning;trajectory optimization}, 
doi={10.1109/HRI.2013.6483603}, 
ISSN={2167-2121}, 
month={March},}

@inproceedings{rakovic2018dataset,
  title={A dataset of head and eye gaze during dyadic interaction task for modeling robot gaze behavior},
  author={Rakovi{\'c}, Mirko and Duarte, Nuno and Tasevski, Jovica and Santos-Victor, Jos{\'e} and Borovac, Branislav},
  booktitle={MATEC Web of Conferences},
  volume={161},
  pages={03002},
  year={2018},
  organization={EDP Sciences}
}

@article{rakovic2018gazedialogue,
author={M. Rakovic and N. Duarte and J. Marques and J. Santos-Victor},
journal={Paper under revision},
title={Modelling the Gaze Dialogue: Non-verbal communication in Human-Human and Human-Robot Interaction},
year={2018},
volume={1},
number={1},
pages={1-12},
doi={},
ISSN={},
month={},}

@InProceedings{duarte2018actionalignment,
author="Duarte, Nuno Ferreira
and Rakovi{\'{c}}, Mirko
and Marques, Jorge
and Santos-Victor, Jos{\'e}",
editor="Leal-Taix{\'e}, Laura
and Roth, Stefan",
title="Action Alignment from Gaze Cues in Human-Human and Human-Robot Interaction",
booktitle="Computer Vision -- ECCV 2018 Workshops",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="197--212",
abstract="Cognitive neuroscience experiments show how people intensify the exchange of non-verbal cues when they work on a joint task towards a common goal. When individuals share their intentions, it creates a social interaction that drives the mutual alignment of their actions and behavior. To understand the intentions of others, we strongly rely on the gaze cues. According to the role each person plays in the interaction, the resulting alignment of the body and gaze movements will be different. This mechanism is key to understand and model dyadic social interactions.",
isbn="978-3-030-11015-4"
}

@article{DEHAIS2011hrhandover,
title = "Physiological and subjective evaluation of a human–robot object hand-over task",
journal = "Applied Ergonomics",
volume = "42",
number = "6",
pages = "785 - 791",
year = "2011",
issn = "0003-6870",
doi = "https://doi.org/10.1016/j.apergo.2010.12.005",
url = "http://www.sciencedirect.com/science/article/pii/S0003687011000044",
author = "Frédéric Dehais and Emrah Akin Sisbot and Rachid Alami and Mickaël Causse",
keywords = "Human–Robot interaction, Robot companion, Physiology, Eye tracking, Human aware planning, Subjective evaluation",
}
